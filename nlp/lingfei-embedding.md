* [Features of similarity (Psychological Review 1977)](http://www.cogsci.ucsd.edu/~coulson/203/tversky-features.pdf)

* [The Computation of Word Associations: Comparing Syntagmatic and Paradigmatic Approaches (COLING 2002)](https://www.aclweb.org/anthology/C02-1007.pdf)

* [Recipe recommendation using ingredient networks (ACM Web Science 2012)](https://dl.acm.org/doi/10.1145/2380718.2380757)

* [Neural Word Embedding as Implicit Matrix Factorization (NIPS 2014)](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf)
  * We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context.

* [A Simple Word Embedding Model for Lexical Substitution (NAACL-HLT 2015)](https://www.aclweb.org/anthology/W15-1501.pdf)
  *  In this work we propose a simple model for lexical substitution, which is based on the popular skip-gram word embedding model.

* [Improving Distributional Similarity with Lessons Learned from Word Embeddings (ACL 2015)](https://www.aclweb.org/anthology/Q15-1016.pdf)
  * We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. 

* [Vector Semantics [Lec Slides 2015]](https://web.stanford.edu/~jurafsky/li15/lec3.vector.pdf)

* [Improving Document Ranking with Dual Word Embeddings (WWW 2016)](https://dl.acm.org/doi/10.1145/2872518.2889361)
  * This paper investigates the popular neural word embedding method Word2vec as a source of evidence in document ranking. In contrast to NLP applications of word2vec, which tend to use only the input embeddings, we retain both the input and the output embeddings, allowing us to calculate a different word similarity that may be more suitable for document ranking. 

* Cooking up Food Embeddings Understanding Flavors in the Recipe-Ingredient Graph (CS224W-Project 2017)

* [The strange geometry of skip-gram with negative sampling](https://www.aclweb.org/anthology/D17-1308.pdf)

* [Linear Algebraic Structure of Word Senses, with Applications to Polysemy (ACL 2018)](https://www.aclweb.org/anthology/Q18-1034.pdf)
  * Here it is shown that multiple word senses re- side in linear superposition within the word embedding and simple sparse coding can re- cover vectors that approximately capture the senses.

* Deconstructing and reconstructing word embedding algorithms (arxiv 2019)

* [【Medium】Unifying Word Embeddings and Matrix Factorization — Part 1](https://medium.com/radix-ai-blog/unifying-word-embeddings-and-matrix-factorization-part-1-cb3984e95141s)

* [Speech and Language Processing-Chapter 6: Vector Semantics and Embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf)